{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Stochastic Methods\n",
    "### Getting Started\n",
    "This tutorial focuses on using stochastic methods to estimate ultimates. \n",
    "\n",
    "Note that a lot of the examples shown here might not be applicable in a real world scenario, and is only meant to demonstrate some of the functionalities included in the package. The user should always exercise their best actuarial judgement, and follow any applicable laws, the Code of Professional Conduct, and applicable Actuarial Standards of Practice.\n",
    "\n",
    "Be sure to make sure your packages are updated. For more info on how to update your pakages, visit [Keeping Packages Updated](https://chainladder-python.readthedocs.io/en/latest/install.html#keeping-packages-updated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black linter, optional\n",
    "%load_ext lab_black\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chainladder as cl\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"pandas: \" + pd.__version__)\n",
    "print(\"numpy: \" + np.__version__)\n",
    "print(\"chainladder: \" + cl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to MackChainladder\n",
    "\n",
    "Like the basic `Chainladder` method, the `MackChainladder` is entirely specified by its selected development pattern. In fact, it is the basic `Chainladder`, but with extra features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clrd = (\n",
    "    cl.load_sample(\"clrd\")\n",
    "    .groupby(\"LOB\")\n",
    "    .sum()\n",
    "    .loc[\"wkcomp\", [\"CumPaidLoss\", \"EarnedPremNet\"]]\n",
    ")\n",
    "\n",
    "cl.Chainladder().fit(clrd[\"CumPaidLoss\"]).ultimate_ == cl.MackChainladder().fit(\n",
    "    clrd[\"CumPaidLoss\"]\n",
    ").ultimate_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Mack's Chainladder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mack = cl.MackChainladder().fit(clrd[\"CumPaidLoss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MackChainladder has the following additional fitted features that the deterministic `Chainladder` does not:\n",
    "\n",
    "- `full_std_err_`:  The full standard error\n",
    "- `total_process_risk_`: The total process error\n",
    "- `total_parameter_risk_`: The total parameter error\n",
    "- `mack_std_err_`: The total prediction error by origin period\n",
    "- `total_mack_std_err_`: The total prediction error across all origin periods\n",
    "\n",
    "Notice these are all measures of uncertainty, but where can they be applied? Let's start by examining the `link_ratios` underlying the triangle between age 12 and 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clrd_first_lags = clrd[clrd.development <= 24][clrd.origin < \"1997\"][\"CumPaidLoss\"]\n",
    "clrd_first_lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple average link-ratio can be directly computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clrd_first_lags.link_ratio.to_frame().mean()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify that the result is the same as the `Development` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.Development(average=\"simple\").fit(clrd[\"CumPaidLoss\"]).ldf_.to_frame().values[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Linear Regression Framework\n",
    "\n",
    "Mack noted that the estimate for the LDF is really just a linear regression fit. In the case of using the `simple` average, it is a weighted regression where the weight is $\\left (\\frac{1}{X}  \\right )^{2}$.\n",
    "\n",
    "Let's take a look at the fitted coefficient and verify that this ties to the direct calculations that we made earlier.\n",
    "With the regression framework in hand, we can get more information about our LDF estimate than just the coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clrd_first_lags.to_frame().values[:, 1]\n",
    "x = clrd_first_lags.to_frame().values[:, 0]\n",
    "\n",
    "model = sm.WLS(y, x, weights=(1 / x) ** 2)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By toggling the weights of our regression, we can handle the most common types of averaging used in picking loss development factors.\n",
    "- For simple average, the weights are $\\left (\\frac{1}{X}  \\right )^{2}$\n",
    "- For volume-weighted average, the weights are $\\left (\\frac{1}{X}  \\right )$\n",
    "- For \"regression\" average, the weights are 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simple average:\")\n",
    "print(\n",
    "    round(\n",
    "        cl.Development(average=\"simple\")\n",
    "        .fit(clrd_first_lags)\n",
    "        .ldf_.to_frame()\n",
    "        .values[0, 0],\n",
    "        10,\n",
    "    )\n",
    "    == round(sm.WLS(y, x, weights=(1 / x) ** 2).fit().params[0], 10)\n",
    ")\n",
    "\n",
    "print(\"Volume-weighted average:\")\n",
    "print(\n",
    "    round(\n",
    "        cl.Development(average=\"volume\")\n",
    "        .fit(clrd_first_lags)\n",
    "        .ldf_.to_frame()\n",
    "        .values[0, 0],\n",
    "        10,\n",
    "    )\n",
    "    == round(sm.WLS(y, x, weights=(1 / x)).fit().params[0], 10)\n",
    ")\n",
    "\n",
    "print(\"Regression average:\")\n",
    "print(\n",
    "    round(\n",
    "        cl.Development(average=\"regression\")\n",
    "        .fit(clrd_first_lags)\n",
    "        .ldf_.to_frame()\n",
    "        .values[0, 0],\n",
    "        10,\n",
    "    )\n",
    "    == round(sm.OLS(y, x, weights=1).fit().params[0], 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression framework is what the `Development` estimator uses to set development patterns. Although we discard the information in the deterministic methods, in the stochastic methods, `Development` has two useful statistics for estimating reserve variability, both of which come from the regression framework. The stastics are `sigma_` and `std_err_` , and they are used by the `MackChainladder` estimator to determine the prediction error of our reserves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = cl.Development(average=\"simple\").fit(clrd[\"CumPaidLoss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.sigma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.std_err_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `std_err_` is calculated as $\\frac{\\sigma}{\\sqrt{N}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(\n",
    "    dev.sigma_.to_frame().transpose()[\"(All)\"].values\n",
    "    / np.sqrt(clrd[\"CumPaidLoss\"].age_to_age.to_frame().count()).values,\n",
    "    4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the regression framework uses the weighting method, we can easily turn \"on and off\" any observation we want using the dropping capabilities such as `drop_valuation` in the `Development` estimator. Dropping link ratios not only affects the `ldf_` and `cdf_`, but also the `std_err_` and `sigma` of the estimates.\n",
    "\n",
    "Can we eliminate the 1988 valuation from our triangle, which is identical to eliminating the first observation from our 12-24 regression fit? Let's calculate the `std_err` for the `ldf_` of ages 12-24, and compare it to the value calculated using the weighted least squares regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clrd[\"CumPaidLoss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(\n",
    "    cl.Development(average=\"volume\", drop_valuation=\"1988\")\n",
    "    .fit(clrd[\"CumPaidLoss\"])\n",
    "    .std_err_.to_frame()\n",
    "    .values[0, 0],\n",
    "    8,\n",
    ") == round(sm.WLS(y[1:], x[1:], weights=(1 / x[1:])).fit().bse[0], 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `sigma_` and `std_err_` in hand, Mack goes on to develop recursive formulas to estimate `parameter_risk_` and `process_risk_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mack.parameter_risk_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mack.process_risk_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption of Independence\n",
    "The Mack model makes a lot of assumptions about independence (i.e. the covariance between random processes is 0). This means that many of the Variance estimates in the `MackChainladder` model follow the form of $Var(A+B) = Var(A)+Var(B)$. \n",
    "\n",
    "First, `mack_std_err_`<sup>2</sup> $=$ `parameter_risk_`<sup>2</sup> $+$ `process_risk_`<sup>2</sup>, the parameter risk and process risk is assumed to be independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mack.parameter_risk_ ** 2 + mack.process_risk_ ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mack.mack_std_err_ ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, `total_process_risk_` <sup>2</sup> $= \\sum_{origin} $ `process_risk_` <sup>2</sup>, the process risk is assumed to be independent between origins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mack.total_process_risk_ ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mack.process_risk_ ** 2).sum(axis=\"origin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, independence is also assumed to apply to the overall standard error of reserves, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mack.parameter_risk_ ** 2 + mack.process_risk_ ** 2).sum(axis=2).sum(axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mack.mack_std_err_ ** 2).sum(axis=2).sum(axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This over-reliance on independence is one of the weaknesses of the `MackChainladder` method. Nevertheless, if the data align with this assumption, then `total_mack_std_err_` is a reasonable esimator of reserve variability.\n",
    "\n",
    "### Mack Reserve Variability\n",
    "The `mack_std_err_` at ultimate is the reserve variability for each `origin` period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mack.mack_std_err_[\n",
    "    mack.mack_std_err_.development == mack.mack_std_err_.development.max()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `summary_` method, we can more easily look at the result of the `MackChainladder` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mack.summary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the paid to date, the estimated reserves, and their standard errors with a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    mack.summary_.to_frame().index.year,\n",
    "    mack.summary_.to_frame()[\"Latest\"],\n",
    "    label=\"Paid\",\n",
    ")\n",
    "plt.bar(\n",
    "    mack.summary_.to_frame().index.year,\n",
    "    mack.summary_.to_frame()[\"IBNR\"],\n",
    "    bottom=mack.summary_.to_frame()[\"Latest\"],\n",
    "    yerr=mack.summary_.to_frame()[\"Mack Std Err\"],\n",
    "    label=\"Reserves\",\n",
    ")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim(0, 1800000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also simulate the (assumed) normally distributed IBNR with `np.random.normal()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibnr_mean = mack.ibnr_.sum()\n",
    "ibnr_sd = mack.total_mack_std_err_.values[0, 0]\n",
    "n_trials = 10000\n",
    "\n",
    "np.random.seed(2021)\n",
    "dist = np.random.normal(ibnr_mean, ibnr_sd, size=n_trials)\n",
    "\n",
    "plt.hist(dist, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODP Bootstrap Model\n",
    "The `MackChainladder` focuses on a regression framework for determining the variability of reserve estimates. An alternative approach is to use the statistical bootstrapping, or sampling from a triangle with replacement to simulate new triangles.\n",
    "\n",
    "Bootstrapping imposes less model constraints than the `MackChainladder`, which allows for greater applicability in different scenarios. Sampling new triangles can be accomplished through the `BootstrapODPSample` estimator. This estimator will take a single triangle and simulate new ones from it. To simulate new triangles randomly from an existing triangle, we specify `n_sims` with how many triangles we want to simulate, and access the `resampled_triangles_` attribute to get the simulated triangles. Notice that the shape of `resampled_triangles_` matches `n_sims` at the first index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = (\n",
    "    cl.BootstrapODPSample(n_sims=10000).fit(clrd[\"CumPaidLoss\"]).resampled_triangles_\n",
    ")\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use `BootstrapODPSample` to transform our triangle into a resampled set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of the ODP Bootstrap is that as our simulations approach infinity, we should expect our mean simulation to converge on the basic `Chainladder` estimate of of reserves.\n",
    "\n",
    "Let's apply the basic chainladder to our original triangle and also to our simulated triangles to see whether this holds true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibnr_cl = cl.Chainladder().fit(clrd[\"CumPaidLoss\"]).ibnr_.sum()\n",
    "ibnr_bootstrap = cl.Chainladder().fit(samples).ibnr_.sum(\"origin\").mean()\n",
    "\n",
    "print(\n",
    "    \"Chainladder's IBNR estimate:\", ibnr_cl,\n",
    ")\n",
    "print(\n",
    "    \"BootstrapODPSample's mean IBNR estimate:\", ibnr_bootstrap,\n",
    ")\n",
    "print(\"Difference $:\", ibnr_cl - ibnr_bootstrap)\n",
    "print(\"Difference %:\", abs(ibnr_cl - ibnr_bootstrap) / ibnr_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Deterministic Methods with Bootstrapped Samples\n",
    "Our `samples` is just another triangle object with all the functionality of a regular triangle.  This means we can apply any functionality we want to our `samples` including any deterministic methods we learned about previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = cl.Pipeline(\n",
    "    [(\"dev\", cl.Development(average=\"simple\")), (\"tail\", cl.TailConstant(1.05))]\n",
    ")\n",
    "pipe.fit(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of a single `ldf_` (and `cdf_`) array across developmental ages, we have 10,000 arrays of `ldf_` (and `cdf_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps.dev.ldf_.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps.dev.ldf_.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps.dev.ldf_.iloc[9999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to look at the varibility of any fitted property used. Let's look at the distribution of the age-to-age factor between age 12 and 24, and compare it against the LDF calculated from the non-bootstrapped triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_ldf = pipe.named_steps.dev.ldf_\n",
    "plt.hist(pipe.named_steps.dev.ldf_.values[:, 0, 0, 0], bins=50)\n",
    "\n",
    "orig_dev = cl.Development(average=\"simple\").fit(clrd[\"CumPaidLoss\"])\n",
    "plt.axvline(orig_dev.ldf_.values[0, 0, 0, 0], color=\"black\", linestyle=\"dashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bootstrap vs Mack\n",
    "We can approximate some of the Mack's parameters calculated using the regression framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_vs_mack = resampled_ldf.std(\"index\").to_frame().T\n",
    "bootstrap_vs_mack.rename(columns={\"(All)\": \"Std_Bootstrap\"}, inplace=True)\n",
    "bootstrap_vs_mack = bootstrap_vs_mack.merge(\n",
    "    orig_dev.std_err_.to_frame().T, left_index=True, right_index=True\n",
    ")\n",
    "bootstrap_vs_mack.rename(columns={\"(All)\": \"Std_Mack\"}, inplace=True)\n",
    "\n",
    "bootstrap_vs_mack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 0.3\n",
    "ages = np.arange(len(bootstrap_vs_mack))\n",
    "\n",
    "plt.bar(\n",
    "    ages - width / 2,\n",
    "    bootstrap_vs_mack[\"Std_Bootstrap\"],\n",
    "    width=width,\n",
    "    label=\"Bootstrap\",\n",
    ")\n",
    "plt.bar(ages + width / 2, bootstrap_vs_mack[\"Std_Mack\"], width=width, label=\"Mack\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xticks(ages, bootstrap_vs_mack.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the `MackChainladder` produces statistics about the mean and variance of reserve estimates, the variance or precentile of reserves would need to be fited using maximum likelihood estimation or method of moments. However, for `BootstrapODPSample` based fits, we can use the empirical distribution from the samples directly to get information about variance or the precentile of the IBNR reserves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibnr = cl.Chainladder().fit(samples).ibnr_.sum(\"origin\")\n",
    "\n",
    "ibnr_std = ibnr.std()\n",
    "print(\"Standard deviation of reserve estimate: \" + f\"{round(ibnr_std,0):,}\")\n",
    "ibnr_99 = ibnr.quantile(q=0.99)\n",
    "print(\"99th percentile of reserve estimate: \" + f\"{round(ibnr_99,0):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the `MackChainladder` reserve distribution compares to the `BootstrapODPSample` reserve distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ibnr.to_frame(), bins=50, label=\"Bootstrap\", alpha=0.3)\n",
    "plt.hist(dist, bins=50, label=\"Mack\", alpha=0.3)\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping with the Bornhuetter-Ferguson Method\n",
    "\n",
    "So far, we've only applied the multiplicative methods (i.e. basic chainladder) in a stochastic context. It is possible to use an expected loss method like the `BornhuetterFerguson` method does. \n",
    "\n",
    "To do this, we will need an exposure vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clrd[\"EarnedPremNet\"].latest_diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing an `apriori_sigma` to the `BornhuetterFerguson` estimator tells it to consider the `apriori` selection itself as a random variable. Fitting a stochastic `BornhuetterFerguson` looks very much like the determinsitic version. Let's assume that the `apriori` is 80% (of `clrd[\"EarnedPremNet\"]`) and its standard deviation is 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cl.BornhuetterFerguson(apriori=0.80, apriori_sigma=0.10)\n",
    "bf.fit(samples, sample_weight=clrd[\"EarnedPremNet\"].latest_diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's restate or sampled triangles so that the upper left portion of the triangle with known values. We will need to start with the `full_triangle_`, then take out the upper left (the simulated values) with `X_`, then add back the actual values from the raw triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restated_triangle = bf.full_triangle_ - bf.X_ + clrd[\"CumPaidLoss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at how a certain origin period developed with the sampled triangles. Let's take a look at origin year 1995."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restated_triangle_1995_df = restated_triangle[\n",
    "    restated_triangle.origin == \"1995\"\n",
    "].to_frame()\n",
    "restated_triangle_1995_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's only graph the first 1,000 simulations. As expected, plotting the expected development of our full triangle over time from the Bootstrap `BornhuetterFerguson` model fans out to greater uncertainty the farther we get from our valuation date. And notice that for 1997 and prior (age 36 and prior), there is no variability as we have restated the simulated triangles with actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    restated_triangle_1995_df.T.reset_index(drop=True).iloc[:, 0:1000],\n",
    "    color=\"blue\",\n",
    "    alpha=0.01,\n",
    ");\n",
    "plt.xticks(\n",
    "    np.arange(0, 12, 1),\n",
    "    [\"12\", \"24\", \"36\", \"48\", \"60\", \"72\", \"84\", \"96\", \"108\", \"120\", \"132\", \"Ult\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "- The Mack method approaches stochastic reserving from a regression point of view\n",
    "- Bootstrap methods approach stochastic reserving from a simulation point of view\n",
    "- When the assumptions of the model are not violated, they will both produce resonably consistent estimates of reserve variability\n",
    "- Mack does impose more assumptions (i.e. constraints) on the reserve estimate making the Bootstrap approach more suitable in a broader set of applciations\n",
    "- Both methods converge to their corresponding deterministic point estimates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
